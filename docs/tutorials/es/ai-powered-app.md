# Tutorial: AplicaciÃ³n con IA usando HOPE Agents

## Objetivo

Construir una aplicaciÃ³n inteligente que aprende automÃ¡ticamente y toma decisiones usando HOPE Agents (Hierarchical Optimized Policy Engine). Los agentes pueden aprender de la experiencia, manejar metas jerÃ¡rquicas, detectar anomalÃ­as y ejecutar acciones autÃ³nomas.

## Prerrequisitos

- Completar el [tutorial de inicio rÃ¡pido](./getting-started.md)
- Conocimientos bÃ¡sicos de aprendizaje por refuerzo
- Familiaridad con conceptos de IA (opcional pero Ãºtil)

## Tiempo estimado

90-120 minutos

---

## Paso 1: Entender HOPE Agents

HOPE (Hierarchical Optimized Policy Engine) combina:

- **Q-Learning**: Aprende valores de estado-acciÃ³n
- **SARSA**: Aprende polÃ­ticas on-policy
- **TD-Learning**: Aprendizaje por diferencia temporal
- **Hierarchical Goals**: Descompone objetivos complejos
- **Predictive Model**: Predice estados futuros
- **Anomaly Detection**: Detecta comportamientos inusuales

**Arquitectura:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           HOPE Agent                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Learning â”‚  â”‚ Goal Solver      â”‚     â”‚
â”‚  â”‚ Engine   â”‚  â”‚ (Hierarchical)   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Predictive Model                  â”‚   â”‚
â”‚  â”‚ - Forecasting                     â”‚   â”‚
â”‚  â”‚ - Anomaly Detection               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Paso 2: Configurar HOPE Agent bÃ¡sico

Crea un nuevo proyecto:

```bash
mkdir aingle-ai-app
cd aingle-ai-app
cargo init
```

AÃ±ade dependencias al `Cargo.toml`:

```toml
[package]
name = "aingle-ai-app"
version = "0.1.0"
edition = "2021"

[dependencies]
hope_agents = { path = "../../crates/hope_agents" }
tokio = { version = "1", features = ["full"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
anyhow = "1"
env_logger = "0.11"
```

Crea tu primer agente:

```rust
// src/main.rs
use hope_agents::{HopeAgent, AgentConfig, Observation, ActionResult};
use std::time::Duration;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    env_logger::init();

    // Configurar agente
    let config = AgentConfig {
        name: "smart_thermostat".to_string(),
        max_memory_bytes: 2 * 1024 * 1024, // 2 MB
        decision_interval: Duration::from_millis(200),
        max_goals: 20,
        learning_enabled: true,
        learning_rate: 0.15,       // Tasa de aprendizaje
        exploration_rate: 0.2,     // ExploraciÃ³n vs explotaciÃ³n
        max_rules: 500,
        sensor_interval: Duration::from_millis(100),
        action_timeout: Duration::from_secs(10),
    };

    println!("ğŸ¤– Creando HOPE Agent: {}", config.name);
    println!("   Learning: {}", config.learning_enabled);
    println!("   Learning rate: {}", config.learning_rate);
    println!("   Exploration: {}%\n", config.exploration_rate * 100.0);

    // Crear agente
    let mut agent = HopeAgent::new(config)?;

    println!("âœ“ Agente creado y listo para aprender\n");

    Ok(())
}
```

**ExplicaciÃ³n de parÃ¡metros:**

- `learning_rate (0.15)`: QuÃ© tan rÃ¡pido aprende (0=nada, 1=instantÃ¡neo)
- `exploration_rate (0.2)`: 20% explora acciones nuevas, 80% usa lo aprendido
- `max_goals (20)`: Puede manejar hasta 20 objetivos simultÃ¡neos
- `decision_interval`: Toma decisiones cada 200ms

---

## Paso 3: Definir observaciones y acciones

Los agentes aprenden mediante observaciones del entorno y ejecutando acciones:

```rust
// src/thermostat.rs
use serde::{Deserialize, Serialize};
use hope_agents::{Observation, Action, ActionResult};

/// Estado del termostato
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ThermostatState {
    pub current_temp: f64,      // Temperatura actual
    pub target_temp: f64,       // Temperatura deseada
    pub humidity: f64,          // Humedad %
    pub occupancy: bool,        // Â¿Hay gente?
    pub time_of_day: u8,        // Hora 0-23
}

impl ThermostatState {
    /// Convertir a observaciÃ³n para el agente
    pub fn to_observation(&self) -> Observation {
        // Discretizar el estado en categorÃ­as
        let temp_diff = (self.target_temp - self.current_temp).round() as i32;
        let humidity_level = (self.humidity / 10.0).round() as u8; // 0-10
        let occupancy_flag = if self.occupancy { 1 } else { 0 };

        Observation {
            state_id: format!(
                "temp_{}_hum_{}_occ_{}_hour_{}",
                temp_diff, humidity_level, occupancy_flag, self.time_of_day
            ),
            features: vec![
                self.current_temp,
                self.target_temp,
                self.humidity,
                occupancy_flag as f64,
                self.time_of_day as f64,
            ],
            metadata: Some(serde_json::json!({
                "temp_diff": temp_diff,
                "comfort_level": self.calculate_comfort(),
            })),
        }
    }

    /// Calcular nivel de confort (0-100)
    fn calculate_comfort(&self) -> f64 {
        let temp_comfort = if (self.current_temp - self.target_temp).abs() < 1.0 {
            100.0
        } else {
            let diff = (self.current_temp - self.target_temp).abs();
            (100.0 - diff * 10.0).max(0.0)
        };

        let humidity_comfort = if (40.0..=60.0).contains(&self.humidity) {
            100.0
        } else {
            let diff = if self.humidity < 40.0 {
                40.0 - self.humidity
            } else {
                self.humidity - 60.0
            };
            (100.0 - diff * 2.0).max(0.0)
        };

        (temp_comfort + humidity_comfort) / 2.0
    }
}

/// Acciones que puede tomar el termostato
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
pub enum ThermostatAction {
    HeatHigh,       // Calentar intenso (+2Â°C/min)
    HeatLow,        // Calentar suave (+0.5Â°C/min)
    Cool,           // Enfriar (-1Â°C/min)
    Fan,            // Solo ventilador (mejorar circulaciÃ³n)
    Off,            // Apagar
}

impl ThermostatAction {
    /// Convertir a Action para el agente
    pub fn to_action(&self) -> Action {
        Action {
            action_id: self.action_name().to_string(),
            parameters: serde_json::to_value(self).unwrap(),
        }
    }

    pub fn action_name(&self) -> &str {
        match self {
            Self::HeatHigh => "heat_high",
            Self::HeatLow => "heat_low",
            Self::Cool => "cool",
            Self::Fan => "fan",
            Self::Off => "off",
        }
    }

    /// Ejecutar acciÃ³n (simulado)
    pub fn execute(&self, state: &mut ThermostatState) -> ActionResult {
        let initial_comfort = state.calculate_comfort();

        match self {
            Self::HeatHigh => {
                state.current_temp += 2.0;
            }
            Self::HeatLow => {
                state.current_temp += 0.5;
            }
            Self::Cool => {
                state.current_temp -= 1.0;
            }
            Self::Fan => {
                // Mejora circulaciÃ³n, reduce humedad ligeramente
                state.humidity = (state.humidity - 2.0).max(0.0);
            }
            Self::Off => {
                // Deriva hacia temperatura ambiente (20Â°C)
                let ambient = 20.0;
                let drift = (ambient - state.current_temp) * 0.1;
                state.current_temp += drift;
            }
        }

        let final_comfort = state.calculate_comfort();
        let comfort_improvement = final_comfort - initial_comfort;

        // Recompensa basada en mejorar confort
        let reward = comfort_improvement / 10.0; // Escalar a [-10, 10]

        ActionResult {
            success: true,
            reward,
            new_observation: state.to_observation(),
            done: false,
            metadata: Some(serde_json::json!({
                "comfort_before": initial_comfort,
                "comfort_after": final_comfort,
                "improvement": comfort_improvement,
            })),
        }
    }
}
```

**ExplicaciÃ³n:**

- **Observation**: Representa el estado actual del entorno
- **Action**: Representa una acciÃ³n que el agente puede ejecutar
- **Reward**: SeÃ±al de aprendizaje (+recompensa = bueno, -recompensa = malo)
- **calculate_comfort()**: FunciÃ³n objetivo que el agente maximiza

---

## Paso 4: Metas jerÃ¡rquicas

HOPE Agents soportan metas jerÃ¡rquicas que se descomponen en submetas:

```rust
// src/goals.rs
use hope_agents::{Goal, GoalPriority};

/// Definir objetivos del termostato
pub fn create_comfort_goals() -> Vec<Goal> {
    vec![
        // Meta principal: Confort Ã³ptimo
        Goal {
            goal_id: "optimal_comfort".to_string(),
            description: "Mantener confort Ã³ptimo (temp + humedad)".to_string(),
            priority: GoalPriority::High,
            parent_goal: None,
            success_condition: Box::new(|obs| {
                // Ã‰xito si comfort > 90%
                if let Some(metadata) = &obs.metadata {
                    if let Some(comfort) = metadata.get("comfort_level") {
                        return comfort.as_f64().unwrap_or(0.0) > 90.0;
                    }
                }
                false
            }),
            subgoals: vec!["maintain_temp".to_string(), "maintain_humidity".to_string()],
        },
        // Submeta 1: Temperatura
        Goal {
            goal_id: "maintain_temp".to_string(),
            description: "Mantener temperatura objetivo Â±1Â°C".to_string(),
            priority: GoalPriority::Medium,
            parent_goal: Some("optimal_comfort".to_string()),
            success_condition: Box::new(|obs| {
                if let Some(metadata) = &obs.metadata {
                    if let Some(diff) = metadata.get("temp_diff") {
                        let diff_val = diff.as_i64().unwrap_or(10);
                        return diff_val.abs() <= 1;
                    }
                }
                false
            }),
            subgoals: vec![],
        },
        // Submeta 2: Humedad
        Goal {
            goal_id: "maintain_humidity".to_string(),
            description: "Mantener humedad 40-60%".to_string(),
            priority: GoalPriority::Medium,
            parent_goal: Some("optimal_comfort".to_string()),
            success_condition: Box::new(|obs| {
                if obs.features.len() > 2 {
                    let humidity = obs.features[2];
                    return (40.0..=60.0).contains(&humidity);
                }
                false
            }),
            subgoals: vec![],
        },
        // Meta secundaria: Eficiencia energÃ©tica
        Goal {
            goal_id: "energy_efficiency".to_string(),
            description: "Minimizar uso de energÃ­a".to_string(),
            priority: GoalPriority::Low,
            parent_goal: None,
            success_condition: Box::new(|_obs| {
                // Siempre activo, prioridad baja
                false
            }),
            subgoals: vec![],
        },
    ]
}
```

**ExplicaciÃ³n:**

- **HierarchÃ­a**: `optimal_comfort` â†’ `maintain_temp` + `maintain_humidity`
- **Prioridades**: High > Medium > Low
- **Success conditions**: Predicados que determinan si se logrÃ³ el objetivo
- El agente trabaja en mÃºltiples metas simultÃ¡neamente

---

## Paso 5: Aprendizaje automÃ¡tico

Entrena el agente con episodios de aprendizaje:

```rust
// src/training.rs
use hope_agents::HopeAgent;
use crate::thermostat::{ThermostatState, ThermostatAction};
use crate::goals::create_comfort_goals;

pub async fn train_agent(agent: &mut HopeAgent, episodes: usize) -> anyhow::Result<()> {
    println!("ğŸ“ Iniciando entrenamiento ({} episodios)...\n", episodes);

    // Cargar objetivos
    let goals = create_comfort_goals();
    for goal in goals {
        agent.add_goal(goal).await?;
    }

    let actions = vec![
        ThermostatAction::HeatHigh,
        ThermostatAction::HeatLow,
        ThermostatAction::Cool,
        ThermostatAction::Fan,
        ThermostatAction::Off,
    ];

    for episode in 0..episodes {
        // Estado inicial aleatorio
        let mut state = ThermostatState {
            current_temp: 15.0 + (rand::random::<f64>() * 10.0), // 15-25Â°C
            target_temp: 21.0,
            humidity: 40.0 + (rand::random::<f64>() * 30.0), // 40-70%
            occupancy: rand::random(),
            time_of_day: (rand::random::<f64>() * 24.0) as u8,
        };

        let mut total_reward = 0.0;
        let max_steps = 50;

        for step in 0..max_steps {
            // Observar estado actual
            let observation = state.to_observation();

            // Decidir acciÃ³n (explora o explota)
            let action_name = agent.decide_action(&observation).await?;

            // Ejecutar acciÃ³n
            let selected_action = actions
                .iter()
                .find(|a| a.action_name() == action_name)
                .unwrap_or(&ThermostatAction::Off);

            let result = selected_action.execute(&mut state);
            total_reward += result.reward;

            // Aprender de la experiencia
            agent.learn(
                &observation,
                &selected_action.to_action(),
                result.reward,
                &result.new_observation,
                result.done,
            ).await?;

            // Terminar si alcanzÃ³ confort Ã³ptimo
            if state.calculate_comfort() > 95.0 {
                break;
            }
        }

        if (episode + 1) % 10 == 0 {
            println!("  Episode {}/{}: Reward = {:.2}", episode + 1, episodes, total_reward);
        }
    }

    println!("\nâœ“ Entrenamiento completado\n");

    // Mostrar polÃ­tica aprendida
    agent.print_policy().await;

    Ok(())
}
```

**ExplicaciÃ³n del proceso:**

1. **InicializaciÃ³n**: Estado aleatorio para explorar diversidad
2. **ObservaciÃ³n**: El agente percibe el estado actual
3. **DecisiÃ³n**: Selecciona acciÃ³n (explora nueva o usa aprendida)
4. **EjecuciÃ³n**: AcciÃ³n modifica el estado
5. **Recompensa**: SeÃ±al de quÃ© tan buena fue la acciÃ³n
6. **Aprendizaje**: Actualiza valores Q(s,a) con la recompensa
7. **IteraciÃ³n**: Repite hasta alcanzar objetivo o timeout

---

## Paso 6: DetecciÃ³n de anomalÃ­as

El modelo predictivo detecta comportamientos anormales:

```rust
// src/anomaly.rs
use hope_agents::HopeAgent;
use crate::thermostat::ThermostatState;

pub async fn detect_anomalies(
    agent: &HopeAgent,
    state: &ThermostatState,
) -> anyhow::Result<bool> {
    let observation = state.to_observation();

    // Predecir siguiente estado esperado
    let prediction = agent.predict_next_state(&observation).await?;

    // Detectar si el estado actual es anÃ³malo
    let is_anomaly = agent.is_anomaly(&observation).await?;

    if is_anomaly {
        println!("âš ï¸  ANOMALÃA DETECTADA:");
        println!("   Temp: {:.1}Â°C (objetivo: {:.1}Â°C)",
            state.current_temp, state.target_temp);
        println!("   Humedad: {:.1}%", state.humidity);
        println!("   Confort: {:.1}%", state.calculate_comfort());

        // Obtener explicaciÃ³n
        if let Some(reason) = agent.explain_anomaly(&observation).await? {
            println!("   RazÃ³n: {}", reason);
        }
    }

    Ok(is_anomaly)
}

/// Monitorear continuamente y alertar anomalÃ­as
pub async fn monitor_anomalies(
    agent: &HopeAgent,
    states: Vec<ThermostatState>,
) -> anyhow::Result<()> {
    println!("ğŸ” Monitoreando anomalÃ­as...\n");

    let mut anomaly_count = 0;

    for (i, state) in states.iter().enumerate() {
        let is_anomaly = detect_anomalies(agent, state).await?;

        if is_anomaly {
            anomaly_count += 1;
        }

        if (i + 1) % 20 == 0 {
            println!("  Procesadas {} lecturas, {} anomalÃ­as detectadas",
                i + 1, anomaly_count);
        }
    }

    println!("\nâœ“ Monitoreo completado");
    println!("  Total anomalÃ­as: {} / {} ({:.1}%)",
        anomaly_count,
        states.len(),
        (anomaly_count as f64 / states.len() as f64) * 100.0
    );

    Ok(())
}
```

**Casos de anomalÃ­as detectadas:**

- Temperatura sube/baja muy rÃ¡pido
- Humedad fuera de rango esperado
- Estado no coincide con predicciÃ³n del modelo
- PatrÃ³n de uso inusual (ej: calefacciÃ³n en verano)

---

## Paso 7: Toma de decisiones autÃ³noma

El agente ejecuta decisiones automÃ¡ticamente:

```rust
// src/autonomous.rs
use hope_agents::HopeAgent;
use crate::thermostat::{ThermostatState, ThermostatAction};
use tokio::time::{interval, Duration};

pub async fn run_autonomous_mode(
    agent: &mut HopeAgent,
    initial_state: ThermostatState,
    duration_secs: u64,
) -> anyhow::Result<()> {
    println!("ğŸ¤– Modo autÃ³nomo activado por {} segundos\n", duration_secs);

    let mut state = initial_state;
    let mut tick_interval = interval(Duration::from_secs(1));
    let end_time = tokio::time::Instant::now() + Duration::from_secs(duration_secs);

    let actions = vec![
        ThermostatAction::HeatHigh,
        ThermostatAction::HeatLow,
        ThermostatAction::Cool,
        ThermostatAction::Fan,
        ThermostatAction::Off,
    ];

    while tokio::time::Instant::now() < end_time {
        tick_interval.tick().await;

        // Observar
        let observation = state.to_observation();

        // Decidir (100% explotaciÃ³n, 0% exploraciÃ³n)
        agent.set_exploration_rate(0.0);
        let action_name = agent.decide_action(&observation).await?;

        // Ejecutar
        let selected_action = actions
            .iter()
            .find(|a| a.action_name() == action_name)
            .unwrap_or(&ThermostatAction::Off);

        let result = selected_action.execute(&mut state);

        // Log
        println!("â”‚ AcciÃ³n: {:?}", selected_action);
        println!("â”‚ Temp: {:.1}Â°C â†’ {:.1}Â°C (objetivo: {:.1}Â°C)",
            state.current_temp - result.new_observation.features[0],
            state.current_temp,
            state.target_temp
        );
        println!("â”‚ Confort: {:.1}%", state.calculate_comfort());
        println!("â”‚ Recompensa: {:.2}", result.reward);
        println!("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

        // Detectar anomalÃ­as en tiempo real
        if agent.is_anomaly(&observation).await? {
            println!("âš ï¸  Comportamiento anÃ³malo detectado");
        }
    }

    println!("\nâœ“ Modo autÃ³nomo finalizado");
    println!("  Temperatura final: {:.1}Â°C", state.current_temp);
    println!("  Confort final: {:.1}%", state.calculate_comfort());

    Ok(())
}
```

**ExplicaciÃ³n:**

- **Exploration rate = 0**: Solo usa lo aprendido, no explora
- **Loop continuo**: Toma decisiones cada segundo
- **AutonomÃ­a completa**: No requiere intervenciÃ³n humana
- **Monitoreo**: Detecta anomalÃ­as en tiempo real

---

## Paso 8: Programa completo

Integra todos los componentes:

```rust
// src/main.rs
mod thermostat;
mod goals;
mod training;
mod anomaly;
mod autonomous;

use hope_agents::{HopeAgent, AgentConfig};
use thermostat::ThermostatState;
use std::time::Duration;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    env_logger::init();

    // 1. Crear agente
    let config = AgentConfig::ai_mode();
    let mut agent = HopeAgent::new(config)?;
    println!("âœ“ HOPE Agent creado\n");

    // 2. Entrenar
    training::train_agent(&mut agent, 100).await?;

    // 3. Probar detecciÃ³n de anomalÃ­as
    let test_states = vec![
        ThermostatState {
            current_temp: 21.0,
            target_temp: 21.0,
            humidity: 50.0,
            occupancy: true,
            time_of_day: 14,
        },
        ThermostatState {
            current_temp: 35.0, // ANOMALÃA: muy caliente
            target_temp: 21.0,
            humidity: 90.0,     // ANOMALÃA: muy hÃºmedo
            occupancy: false,
            time_of_day: 3,
        },
    ];

    anomaly::monitor_anomalies(&agent, test_states).await?;

    // 4. Ejecutar modo autÃ³nomo
    let initial_state = ThermostatState {
        current_temp: 18.0, // FrÃ­o
        target_temp: 21.0,
        humidity: 55.0,
        occupancy: true,
        time_of_day: 10,
    };

    autonomous::run_autonomous_mode(&mut agent, initial_state, 60).await?;

    // 5. Guardar agente entrenado
    agent.save_to_file("thermostat_agent.json").await?;
    println!("\nâœ“ Agente guardado en thermostat_agent.json");

    Ok(())
}
```

---

## Resultado esperado

```
âœ“ HOPE Agent creado

ğŸ“ Iniciando entrenamiento (100 episodios)...

  Episode 10/100: Reward = 23.45
  Episode 20/100: Reward = 31.20
  Episode 30/100: Reward = 38.67
  ...
  Episode 100/100: Reward = 45.89

âœ“ Entrenamiento completado

ğŸ“Š PolÃ­tica aprendida:
  Estado: temp_-3_hum_5_occ_1_hour_10 â†’ AcciÃ³n: heat_high (Q=8.5)
  Estado: temp_0_hum_5_occ_1_hour_10 â†’ AcciÃ³n: off (Q=9.2)
  Estado: temp_+2_hum_5_occ_1_hour_10 â†’ AcciÃ³n: cool (Q=7.8)

ğŸ” Monitoreando anomalÃ­as...

âš ï¸  ANOMALÃA DETECTADA:
   Temp: 35.0Â°C (objetivo: 21.0Â°C)
   Humedad: 90.0%
   Confort: 15.2%
   RazÃ³n: Temperatura fuera de rango esperado (+3 Ïƒ)

âœ“ Monitoreo completado
  Total anomalÃ­as: 1 / 2 (50.0%)

ğŸ¤– Modo autÃ³nomo activado por 60 segundos

â”‚ AcciÃ³n: HeatLow
â”‚ Temp: 18.0Â°C â†’ 18.5Â°C (objetivo: 21.0Â°C)
â”‚ Confort: 72.5%
â”‚ Recompensa: 2.30
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ AcciÃ³n: HeatLow
â”‚ Temp: 18.5Â°C â†’ 19.0Â°C (objetivo: 21.0Â°C)
â”‚ Confort: 80.0%
â”‚ Recompensa: 0.75
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
...

âœ“ Modo autÃ³nomo finalizado
  Temperatura final: 21.2Â°C
  Confort final: 98.5%

âœ“ Agente guardado en thermostat_agent.json
```

---

## Troubleshooting comÃºn

### Agente no aprende

**Problema:** Recompensas siempre bajas, no mejora.

**SoluciÃ³n:**
```rust
config.learning_rate = 0.3;      // Aumentar (mÃ¡s agresivo)
config.exploration_rate = 0.3;   // MÃ¡s exploraciÃ³n
// Entrenar mÃ¡s episodios
train_agent(&mut agent, 500).await?;
```

### Memoria insuficiente

**Problema:** Error "Out of memory".

**SoluciÃ³n:**
```rust
config.max_memory_bytes = 512 * 1024;  // Reducir a 512 KB
config.max_rules = 100;                // Menos reglas
```

### AnomalÃ­as no se detectan

**Problema:** No detecta estados anormales.

**SoluciÃ³n:**
```rust
// Entrenar mÃ¡s para mejorar modelo predictivo
train_agent(&mut agent, 200).await?;

// Ajustar threshold de anomalÃ­a
agent.set_anomaly_threshold(2.5); // MÃ¡s sensible (default: 3.0)
```

---

## PrÃ³ximos pasos

1. **[Integrar con IoT](./iot-sensor-network.md)**: Controlar sensores reales
2. **[Consultas semÃ¡nticas](./semantic-queries.md)**: Consultar decisiones del agente
3. **[Persistencia](./getting-started.md)**: Guardar experiencias en el DAG
4. **ProducciÃ³n**: Deploy en edge devices con AIngle minimal

---

## Conceptos clave aprendidos

- **HOPE Agent**: Agente de aprendizaje jerÃ¡rquico
- **Q-Learning**: Aprende valores de estado-acciÃ³n
- **Exploration vs Exploitation**: Balance entre explorar y usar lo aprendido
- **Hierarchical Goals**: Descomponer objetivos complejos
- **Anomaly Detection**: Detectar comportamientos inusuales
- **Autonomous Decision-making**: Toma decisiones sin intervenciÃ³n humana

---

## Referencias

- [HOPE Agents Implementation](../../crates/hope_agents/IMPLEMENTATION_SUMMARY.md)
- [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)
- [Q-Learning Tutorial](https://en.wikipedia.org/wiki/Q-learning)
- [Hierarchical Reinforcement Learning](https://people.cs.umass.edu/~mahadeva/papers/hrl.pdf)
